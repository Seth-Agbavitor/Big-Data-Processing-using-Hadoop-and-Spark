{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .master('yarn') to run on the cluster, or .master('local[*]') to run locally\n",
    "#spark = SparkSession.builder.appName(\"DataSchema\").master('local[*]').getOrCreate()\n",
    "builder = SparkSession.builder.appName(\"LiquorSale\").master('yarn') \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Disable logging\n",
    "import logging\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logger.LogManager.getLogger(\"org\").setLevel(logger.Level.OFF)\n",
    "logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.OFF)\n",
    "\n",
    "spark.conf.set(\"spark.driver.log.level\", \"OFF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data schema and enforce schema\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "custom_schema = StructType([\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Store\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Zip\", IntegerType(), True),\n",
    "    StructField(\"CountyNumber\", IntegerType(), True),\n",
    "    StructField(\"County\", StringType(), True),\n",
    "    StructField(\"Category\", IntegerType(), True),\n",
    "    StructField(\"CategoryName\", StringType(), True),\n",
    "    StructField(\"VendorNumber\", IntegerType(), True),\n",
    "    StructField(\"VendorName\", StringType(), True),\n",
    "    StructField(\"ItemNumber\", IntegerType(), True),\n",
    "    StructField(\"ItemDescription\", StringType(), True),\n",
    "    StructField(\"StateBottleCost\", DoubleType(), True),\n",
    "    StructField(\"StateBottleRetail\", DoubleType(), True),\n",
    "    StructField(\"BottleSold\", IntegerType(), True),\n",
    "    StructField(\"SaleDollars\", DoubleType(), True),\n",
    "    StructField(\"VolumeSoldLiters\", DoubleType(), True),\n",
    "    StructField(\"VolumeSoldGallons\", DoubleType(), True)\n",
    "])\n",
    "# Read the CSV file and cache the DataFrame\n",
    "df = spark.read.csv(\"/dis_materials/liqour1\", header=False, schema=custom_schema).cache()\n",
    "# Unpersist the DataFrame after using it\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING AND PREPROCESSING \n",
    "# Get the number of rows and columns\n",
    "num_rows = df.count()\n",
    "num_cols = len(df.columns)\n",
    "print(\"There are {:,} rows and {} columns.\\n\".format(num_rows, num_cols))\n",
    "\n",
    "# Show descriptive statistics for selected columns\n",
    "df.select('Date', 'City', 'CategoryName', 'ItemDescription').describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "num_rows = df.count()\n",
    "num_cols = len(df.columns)\n",
    "\n",
    "# Show descriptive statistics for selected columns\n",
    "df.select('Date', 'City', 'CategoryName', 'ItemDescription').describe().show()\n",
    "\n",
    "# Print the results\n",
    "print(\"There are {:,} rows and {} columns.\\n\".format(num_rows, num_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING AND PREPROCESSING  (BASELINE CODE)\n",
    "# How many rows and columns does our data have?  \n",
    "num_rows = df.count()\n",
    "num_cols = len(df.columns)\n",
    " \n",
    "\n",
    "# Show descriptive statistics for selected columns\n",
    "df.select('Date', 'City', 'CategoryName', 'ItemDescription').describe().show()\n",
    "\n",
    "# Print the results\n",
    "print(\"There are {:,} rows and {} columns.\\n\".format(num_rows, num_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING AND PREPROCESSING  (OPTIMIZATION)\n",
    "# How many rows and columns does our data have?  \n",
    "num_rows = df.count()\n",
    "num_cols = len(df.columns)\n",
    "\n",
    "# Repartition the DataFrame to improve performance\n",
    "df = df.repartition(4) \n",
    "\n",
    "# Show descriptive statistics for selected columns\n",
    "df.select('Date', 'City', 'CategoryName', 'ItemDescription').describe().show()\n",
    "\n",
    "# Print the results\n",
    "print(\"There are {:,} rows and {} columns.\\n\".format(num_rows, num_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City and CategoryName data don't seem to have the same amount of rows as Date and ItemDescription. \n",
    "from pyspark.sql.functions import when, sum\n",
    "\n",
    "# Sum the number of null values in City\n",
    "city_nulls = df.select(sum(when(df['City'].isNull(), 1).otherwise(0)).alias('City_Nulls')).collect()[0]['City_Nulls']\n",
    "print(city_nulls)\n",
    "\n",
    "# Sum the number of null values in CategoryName\n",
    "catname_nulls = df.select(sum(when(df['CategoryName'].isNull(), 1).otherwise(0)).alias('CategoryName_Nulls')).collect()[0]['CategoryName_Nulls']\n",
    "print(catname_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date column has a value \"Da\" so we filter out records with invalid month value\n",
    "df = df.filter(df['Month'] != 'Da')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all rows with missing values\n",
    "df=df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 10 most frequent values in the 'City' column to check if City values are uniform \n",
    "#checking the columns values of City and CategoryName (BASELINE CODE)\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Print top 10 most frequent values in the 'City' column\n",
    "df.groupBy('City') \\\n",
    "     .pivot('City') \\\n",
    "     .count() \\\n",
    "     .orderBy(desc('City')) \\\n",
    "     .limit(10) \\\n",
    "     .show()\n",
    "\n",
    "# Print a separator line\n",
    "print('\\n' + '-'*40 + '\\n')\n",
    "\n",
    "# Print top 10 most frequent values in the 'Category Name' column\n",
    "df.groupBy('CategoryName') \\\n",
    "     .pivot('CategoryName') \\\n",
    "     .count() \\\n",
    "     .orderBy(desc('CategoryName')) \\\n",
    "     .limit(10) \\\n",
    "     .show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top 10 most frequent values in the 'City' column to check if City values are uniform \n",
    "#checking the columns values of City and CategoryName (OPTIMIZATION)\n",
    "\n",
    "from pyspark.sql.functions import countDistinct, desc\n",
    "df.select('City') \\\n",
    "     .groupBy('City') \\\n",
    "     .agg(countDistinct('City').alias('count')) \\\n",
    "     .orderBy(desc('City')) \\\n",
    "     .limit(10) \\\n",
    "     .show()\n",
    "\n",
    "# Print a separator line\n",
    "print('\\n' + '-'*40 + '\\n')\n",
    "\n",
    "# Print top 10 most frequent values in the 'Category Name' column\n",
    "df.select('CategoryName') \\\n",
    "     .groupBy('CategoryName') \\\n",
    "     .agg(countDistinct('CategoryName').alias('count')) \\\n",
    "     .orderBy(desc('CategoryName')) \\\n",
    "     .limit(10) \\\n",
    "     .show()\n",
    "#great, capitalization  is uniform and no data is listed twice or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the results into spark delta table by removing duplicate\n",
    "df.write.option(\"overwriteSchema\", \"true\").mode(\"overwrite\").format(\"delta\").saveAsTable(\"liqourtable\", path=\"hdfs:///dis_materials/l3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IOWA LIQUOR DATA ANALYSIS WITH SPARK DATAFRAME API\n",
    "# ANALYSIS 1: WHICH TOP 5 CITIES HAS THE MOST LIQUOR PURCHASE \n",
    "#(BASELINE CODE)\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by city and count number of rows, then cache the data in memory for faster access\n",
    "df_sales_count = df.groupBy('City').count()\n",
    "\n",
    "# Order by count in descending order, then take the top 5\n",
    "top_5_most_sales = df_sales_count.orderBy(desc('count')).limit(5)\n",
    "\n",
    "top_5_most_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IOWA LIQUOR DATA ANALYSIS WITH SPARK DATAFRAME API\n",
    "# ANALYSIS 1: WHICH TOP 5 CITIES HAS THE MOST LIQUOR PURCHASE \n",
    "# (OPTIMIZATION)\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by city and count number of rows, then cache the data in memory for faster access\n",
    "df_sales_count = df.groupBy('City').count().cache()\n",
    "\n",
    "# Order by count in descending order, then take the top 5\n",
    "top_5_most_sales = df_sales_count.orderBy(desc('count')).limit(5)\n",
    "\n",
    "top_5_most_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS 2: DURING WHICH MONTH IS LQUOR SOLD THE MOST?\n",
    "\n",
    "from pyspark.sql.functions import count, desc, substring, split\n",
    "\n",
    "# extract month from 'Date' and store as a new column 'Month'\n",
    "df = df.withColumn('Month', substring(split('Date', '/')[0], 1, 2))\n",
    "\n",
    "# group by month and count the total sales in each month, and sort results in  descending order\n",
    "top_months = df.groupBy('Month').agg(count('*').alias('total_sales')).orderBy(desc('total_sales'))\n",
    "top_months.show()\n",
    "# NB. there are no records for month 7 and month 8 in our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS 2: visualizing the sales by Month with a line plot\n",
    "from pyspark.sql.functions import count, desc, substring, split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# extract month from date and store as a new column 'Month'\n",
    "df = df.withColumn('Month', substring(split('Date', '/')[0], 1, 2))\n",
    "\n",
    "# filter out records with invalid month value 'Da'\n",
    "df = df.filter(df['Month'] != 'Da')\n",
    "\n",
    "# group by month and count the total sales in each month, and sort in ascending order\n",
    "top_months = df.groupBy('Month').agg(count('*').alias('total_sales')).orderBy('Month')\n",
    "\n",
    "# collect the data as a list of Row objects\n",
    "data = top_months.collect()\n",
    "\n",
    "# convert to Pandas DataFrame for plotting\n",
    "sales_by_month_pd = pd.DataFrame(data, columns=['Month', 'total_sales'])\n",
    "\n",
    "# create line plot of sales by month\n",
    "plt.plot(sales_by_month_pd['Month'], sales_by_month_pd['total_sales'], linewidth=2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 209:===================================================> (392 + 2) / 400]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  ItemDescription   count\n",
      "0            TITOS HANDMADE VODKA  133532\n",
      "1       FIREBALL CINNAMON WHISKEY  120427\n",
      "2                    BLACK VELVET  115621\n",
      "3                   HAWKEYE VODKA  103157\n",
      "4  CAPTAIN MORGAN ORIGINAL SPICED   77107\n",
      "5                     CROWN ROYAL   68596\n",
      "6         CROWN ROYAL REGAL APPLE   67859\n",
      "7                  SMIRNOFF 80PRF   55944\n",
      "8                SEAGRAMS 7 CROWN   53261\n",
      "9                        JIM BEAM   53155\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "                CategoryName   count\n",
      "0            AMERICAN VODKAS  811788\n",
      "1          CANADIAN WHISKIES  481656\n",
      "2  STRAIGHT BOURBON WHISKIES  393641\n",
      "3            WHISKEY LIQUEUR  299874\n",
      "4    AMERICAN FLAVORED VODKA  252559\n",
      "5                 SPICED RUM  226778\n",
      "6           BLENDED WHISKIES  222259\n",
      "7         100% AGAVE TEQUILA  192765\n",
      "8          AMERICAN SCHNAPPS  172338\n",
      "9             COCKTAILS /RTD  171029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ANALYSIS 3 (BASELINE CODE): WHICH 10 BRANDS AND TOP 10 LIQUOR TYPES ARE MOST POPULAR IN IOWA\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Get top 10 brands by sales\n",
    "top_brands = df.groupBy('ItemDescription').count().orderBy(desc('count')).limit(10).toPandas()\n",
    "\n",
    "# Get top 10 categories by sales\n",
    "top_types = df.groupBy('CategoryName').count().orderBy(desc('count')).limit(10).toPandas()\n",
    "\n",
    "print(top_brands)\n",
    "print('\\n' + '-'*40 + '\\n')\n",
    "print(top_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 221:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  ItemDescription   count\n",
      "0            TITOS HANDMADE VODKA  133532\n",
      "1       FIREBALL CINNAMON WHISKEY  120427\n",
      "2                    BLACK VELVET  115621\n",
      "3                   HAWKEYE VODKA  103157\n",
      "4  CAPTAIN MORGAN ORIGINAL SPICED   77107\n",
      "5                     CROWN ROYAL   68596\n",
      "6         CROWN ROYAL REGAL APPLE   67859\n",
      "7                  SMIRNOFF 80PRF   55944\n",
      "8                SEAGRAMS 7 CROWN   53261\n",
      "9                        JIM BEAM   53155\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "                CategoryName   count\n",
      "0            AMERICAN VODKAS  811788\n",
      "1          CANADIAN WHISKIES  481656\n",
      "2  STRAIGHT BOURBON WHISKIES  393641\n",
      "3            WHISKEY LIQUEUR  299874\n",
      "4    AMERICAN FLAVORED VODKA  252559\n",
      "5                 SPICED RUM  226778\n",
      "6           BLENDED WHISKIES  222259\n",
      "7         100% AGAVE TEQUILA  192765\n",
      "8          AMERICAN SCHNAPPS  172338\n",
      "9             COCKTAILS /RTD  171029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ANALYSIS 3 (OPTIMIZATION): WHICH 10 BRANDS AND TOP 10 LIQUOR TYPES ARE MOST POPULAR IN IOWA\n",
    "# Repartition the DataFrame before calling groupBy function\n",
    "df_repartitioned = df.repartition('CategoryName', 'ItemDescription')\n",
    "\n",
    "# Get top 10 brands by sales\n",
    "top_brands = df_repartitioned.groupBy('ItemDescription').count().orderBy(desc('count')).limit(10).toPandas()\n",
    "\n",
    "# Get top 10 categories by sales\n",
    "top_types = df_repartitioned.groupBy('CategoryName').count().orderBy(desc('count')).limit(10).toPandas()\n",
    "\n",
    "print(top_brands)\n",
    "print('\\n' + '-'*40 + '\\n')\n",
    "print(top_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis 3: visualizing the top 10 Brand by Sale and the top 10 Liquor Types by Sale with a bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create bar chart for top 10 brands\n",
    "plt.bar(top_brands['ItemDescription'], top_brands['count'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Brand')\n",
    "plt.ylabel('Number of Sales')\n",
    "plt.title('Top 10 Brands by Sales')\n",
    "plt.show()\n",
    "\n",
    "# Create bar chart for top 10 liquor types\n",
    "plt.bar(top_types['CategoryName'], top_types['count'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Liquor Type')\n",
    "plt.ylabel('Number of Sales')\n",
    "plt.title('Top 10 Liquor Types by Sales')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANALYSIS 4: DO SOME CITIES PREFER CERTAIN LIQUORS OVER OTHERS?\n",
    "# We will now analyze the drinking preferences of the residents in Ames and Iowa City, \n",
    "#which are two of the largest college towns in Iowa Specifically,\n",
    "# we will determine the most popular type of liquor in each city based on the available data.\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# group by CategoryName and count the sales in AMES\n",
    "top_type_ames = df.filter(df['City'] == 'AMES') \\\n",
    "                  .groupBy('CategoryName') \\\n",
    "                  .agg(count('*').alias('sales_count')) \\\n",
    "                  .orderBy('sales_count', ascending=False) \\\n",
    "                  .limit(4) \\\n",
    "                  .toPandas()\n",
    "\n",
    "# group by Category Name and count the sales in IOWA CITY\n",
    "top_type_ia_city = df.filter(df['City'] == 'IOWA CITY') \\\n",
    "                     .groupBy('CategoryName') \\\n",
    "                     .agg(count('*').alias('sales_count')) \\\n",
    "                     .orderBy('sales_count', ascending=False) \\\n",
    "                     .limit(4) \\\n",
    "                     .toPandas()\n",
    "\n",
    "print(top_type_ames.head(4))\n",
    "print('\\n' + '-'*40 + '\\n')\n",
    "print(top_type_ia_city.head(4))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fascinating! The top four preferred types of alcoholic beverages are common in both cities, although the order of preference varies. \n",
    "# However, th ouputs suggests that both Ames and Iowa City residents prefer vodka more than other alcoholic beverages "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
